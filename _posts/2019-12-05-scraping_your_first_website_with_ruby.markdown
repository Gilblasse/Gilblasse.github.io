---
layout: post
title:      "Scraping Your First Website With Ruby"
date:       2019-12-06 00:08:01 +0000
permalink:  scraping_your_first_website_with_ruby
---

The first time you start your ruby application, you will find a blank canvas. There is no file structure to work with. While this can create an unsetteling discomfort never forget dont panic. You're not alone in this, we're all human, and we share similar experiences. Don't be afraid to make a mistake, even though it may be expensive, but always face your fear with a smile.

Now,  there are two options for getting started. We can re-create all of our files manually or build our files through a ruby bundler gem. Bundler manages gem dependency in our framework, but today we're going to use it to build our own ruby gem. The greatest advantage of using gems is their ease of reusability. All we have to do is install then require it.

ex:
```
gem install pry
```
```
require 'pry'
```

Until we transform our ruby file into a gem, we must first search if we have a bundler.
```
bundle -v
```

If the bundler is not available, we must test it with the following code line:
```
gem install bundler
```

You will find multiple files added to your file directory at this stage. Hooray no more blank canvas. Now it's time to continue setting up our environment.

![](https://media.giphy.com/media/8NkrBNiXWYhKU/giphy.gif)



Let's add an additional folder to the file directory called `Config` and create an `enviorment.rb` in this folder. This directory will be responsible for containing all of our gems, external data, encoding, and everything else we need to configure our request.
```
require 'open-uri'
require 'nokogiri'

group :development, :test do
  gem 'pry'
end
```

We're going to start setting up our bin file at this point. First, we'll add our she bang text at the top of the page:
```
#!/usr/bin/env ruby
```
This will force a path lookup to find the location of our ruby executable. Then we will require our path to the file `environment.rb.` The last thing we're going to add to this file to make it complete is a method generated by one of our classes or modules to help execute our gem.

```
require_relative './config/environment.rb'
cli = CLI.new
cli.run
```
```
require_relative './config/environment.rb'
cli = BestAmazonDeals::CLI.new
cli.run
```

Now let's make sure it works. We will perform a simple task by specifying a method called `run`. This method will use a stdout (standard output) to print the word `running` to the terminal.

```
module BestAmazonDeals

	   class CLI
		
		     def run
		     	   puts "running"
		     end
		
	  	end
			
	end
```

```
#=>  running
```

Hooray it works, we can now focus on building or gem core functionality.

![](https://media.giphy.com/media/VTYGl4rw4HUmA/giphy.gif)


# Plan Plan Plan

I can't stress that fact enough. The number 1 rule of thumb in begining any project is to plan. Especailly when your scraping content to your CLI gem. You will find without a well written plan its easy to ver away from your original ideas. You'll begin small then jump into larger uncharrted territories with every step adding additional more complex features. This process would quickly overwelm you and eventually you'll feel the preasure to quit. It's always best practice to start with a clear vision of your core elements, despite its rudemantary form executing mundane task.  Eventually, with time, we can add more complex alogrithms and features to make our code more robust. When in doubt remember slow and stead wins the race.  

For more information on tools you can use during your planning process feel free to [visit this site](https://mindsea.com/tools-app-development/).

Once we decide exactly our vision for our CLI gem  we can get started scraping our website with Nokogiri and Open uri. 
```
html = open("https://www.amazon.com/s?k=DEALS")
doc = Nokogiri::HTML(html)
```



# What Is Scraping
Web Scraping (crawling) is the process of retrieve content from either one or more sites, in order to accurately deeping a users understanding of a paticular topic or experiance through your program. Some might wonder why can't we use an API or RSS feed instead. While this is defenently a valuable option some websites, even in the year 2019, do not provide tools to easily extracting information. 

Now while web scraping  is not illegal. There are guiding principles that must be followed. If not you can easily find yourself involved in some sticky situtaions. Before scraping a website always read through the Terms of Service (ToS) or Copyright details to see if theres any disclaimers for scraping content.

For more information visit: [6 Misunderstandings About Web Scraping](https://www.import.io/post/6-misunderstandings-about-web-scraping/)

# Begin Scraping Your First Webpage
![](https://media.giphy.com/media/aNqEFrYVnsS52/giphy.gif)

You've probably noticed this before, but we'll need to download Nokogiri to begin parsing through our web content, and Open-Uri to get our html document  We can also use Watir, when using with a headless ui, to perform the same task. 

step 1: 
    Add  Nokogiri and Open-Uri to gem file then use bundle install. We can then require our gems from our   `enviorment.rb`  file.
		
		```
		   gem "nokogiri", "~> 1.10"
       gem "open_uri_redirections", "~> 0.2.1"
    ```
		
		```
		require 'nokogiri'
		require 'open_uri'
		```
		
step 2: 
      Create a separate file in the lib folder called `scraper.rb.` Inside this file, we're going to set up our new class named Scraper. Here we are going to define a method called scrap or (grab doc). The sole purpose of this method is to grab the html document and embed it with Nokogiri elements so that the data we want can be easily retrieved.
			
			```
			   class Scraper
				 
				        def scrape
	                  	amazon_url = "https://www.amazon.com"
	                   	html = open("#{amazon_url}#{@page}")
		                  @doc = Nokogiri::HTML(html)
	              end
								 
				 end
			```


step 3:
Create another file in your lib folder called 'deals.rb'  this file while initialize each instance of deal. At this point we need to write down our definition of a deal and create its attributes. Usually, we define a deal as an item with an original and discounted price tag.  We also want to save each deal item that gets initalize. The responsibiltiy would be for this class to remeber all of its instances using a class variable called @@all. This means we must set our @@all class variable to an empty array. This way we can push each new instance into it. In addition, we need a class reader method to reveal all our  instances through the class variable @@all.

```
class Deal
	attr_accessor :item_name, :discounted_price, :originial_price
	@@all = []


	def initialize(deal_item_hash)
		
		@@all << self
		
	end
	
	def self.all
	      @@all
	end

```

We need a way to save our values from our hash to our instance variables at this stage. The're a few ways to acheive this...  We can use the send method which executes a method by simply calling its name in a string fromat.  Or we can call each instance variable and set it to the associated hash.


```

class Deal
	attr_accessor :item_name, :discounted_price, :originial_price
	@@all = []


	def initialize(deal_item_hash)
	    
			deal_item_hash.each do |k,v| 
			      self.send("#{k}=",v)
		   end
		
		   @@all << self
		
	end
	
	def self.all
	      @@all
	end
 
```


Step 4 
        Now we're going to revisit our scraper class and create a method to scrape our first page with our expected attributes.	Once the method is madewe will insert a binding pry outside our loop.towards the end. Pry is a runtime developer console. It pry's into the current binding of our code. At runtime, the line where binding pry is found will be interpreted by your interpreter and will freeze. Your terminal will become a REPL (Read, Evaluate, Print, Loop) which allows us to interact with our ruby code in the terminal
				
				```
				 
			   class Scraper
				 
				        def scrape
	                  	amazon_url = "https://www.amazon.com"
	                   	html = open("#{amazon_url}#{@page}")
		                  @doc = Nokogiri::HTML(html)
	              end
								
								
								def first_page
		                     @page = "/s?k=DEALS"
		                      scrape

		                      all_deals = @doc.css(".s-search-results .s-result-item")
		                      all_deals .each do |deal|
			                               hash = {
					                                            	item_name: deal.at.css(.sg-row:nth-child(2) div:nth-child(3) .a-spacing-top-small h2).text
					                                            	discounted_price: deal.at.css(.sg-row:nth-child(2) div:nth-child(4) .a-price span).text
					                                             	originial_price: deal.at.css(.sg-row:nth-child(2) div:nth-child(4).a-text-price.a-offscreen).text
					                                					}
			                                Deal.new(hash)
		                        end 
														
														binding.pry
	                end
								 
				 end
	
				```
				
				

				
				
				Step 5 
					 Finally lets test your `Deal` class methods to see if we retrieve the correct information. Inside our terminal we need to type ruby followed by the name of our bin path filename. to execute our gem.
					 
					 ```
					           18:29:23] (master) ruby-music-library-cli-online-web-ft-110419
               // â™¥		ruby bin/console	
							 
							   pry(1) =>   Deal.all.first.item_name
								 #=>   Fire HD 8 Kids Edition Tablet, 8" HD Display, 32 GB, Blue Kid-Proof Case
								 
								 pry(1) =>   Deal.all.first.discounted_price
								 #=> $79.99
								 
								 
								 pry(1) =>   Deal.all.first.name.originial_price
								 #=> $129.99
		
							 
					 ```
				
				
				HOORAY IT WORKS !!!  You  just scraped your first website with ruby...  CELEBRATE !!!
				
				![](https://media.giphy.com/media/9LXNvkZPYZ1QuH6ws2/giphy.gif)

